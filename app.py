from datetime import datetime
import time
from html import parser
import os
import json
import argparse
import logging

from typing import Optional, List, Dict
from flask import Flask, request, jsonify, render_template_string
import pandas as pd
from providers.chatgpt import ChatGPTProvider
from providers.deepseek import DeepSeekProvider
from rag.retrieve import Retriever
from rag.prompts import generate_system_prompt
from eval.evaluate import evaluate_response

from metrics_logger import metrics_logger

# HTML Template para la interfaz web
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html>
<head>
    <title>UFRO Assistant</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
        .container { max-width: 800px; margin: auto; background: white; padding: 20px; border-radius: 10px; }
        h1 { color: #333; }
        .input-group { margin: 20px 0; }
        input, select, button { padding: 10px; margin: 5px; font-size: 16px; }
        input { width: 60%; }
        button { background-color: #4CAF50; color: white; border: none; cursor: pointer; }
        button:hover { background-color: #45a049; }
        .response { background-color: #f9f9f9; padding: 15px; margin: 20px 0; border-radius: 5px; }
        .citations { font-size: 14px; color: #666; margin-top: 10px; }
        .loading { display: none; color: #666; }
        .metrics-button { 
            background: #3498db; 
            color: white; 
            padding: 8px 15px; 
            text-decoration: none; 
            border-radius: 5px; 
            font-size: 14px;
            float: right;
            margin-top: -50px;
        }
        .metrics-button:hover {
            background: #2980b9;
        }
        .header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header-container">
            <h1>UFRO ChatBot</h1>
            <a href="/metrics" class="metrics-button">üìä Ver M√©tricas</a>
        </div>
        
        <div class="input-group">
            <input type="text" id="query" placeholder="Ingrese su consulta sobre normativa UFRO..." onkeypress="handleKeyPress(event)">
            <select id="provider">
                <option value="chatgpt">ChatGPT</option>
                <option value="deepseek">DeepSeek</option>
            </select>
            <button onclick="sendQuery()">Consultar</button>
        </div>
        <div class="loading" id="loading">Procesando consulta...</div>
        <div id="response"></div>
    </div>
    
    <script>
        function handleKeyPress(event) {
            if (event.key === 'Enter') {
                sendQuery();
            }
        }
        
        async function sendQuery() {
            const query = document.getElementById('query').value;
            const provider = document.getElementById('provider').value;
            const loading = document.getElementById('loading');
            const responseDiv = document.getElementById('response');
            
            if (!query) {
                alert('Por favor, ingrese una consulta');
                return;
            }
            
            loading.style.display = 'block';
            responseDiv.innerHTML = '';
            
            try {
                const response = await fetch('/api/query', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({query, provider})
                });
                
                const data = await response.json();
                
                if (data.error) {
                    responseDiv.innerHTML = '<div class="response">Error: ' + data.error + '</div>';
                } else {
                    // Formatear la respuesta con mejor presentaci√≥n
                    let citationsHtml = '';
                    if (data.citations && data.citations.length > 0) {
                        citationsHtml = `<div class="citations">
                            <strong>üìö Referencias:</strong><br>
                            ${data.citations.join('<br>')}
                        </div>`;
                    }
                    
                    responseDiv.innerHTML = `
                        <div class="response">
                            <strong>ü§ñ Respuesta:</strong><br>
                            ${data.answer.replace(/\\n/g, '<br>')}
                            ${citationsHtml}
                            <div style="font-size: 12px; color: #888; margin-top: 10px;">
                                ‚è±Ô∏è Latencia: ${data.latency}s | üöÄ Proveedor: ${data.provider}
                            </div>
                        </div>
                    `;
                }
            } catch (error) {
                responseDiv.innerHTML = '<div class="response">‚ùå Error de conexi√≥n con el servidor</div>';
            } finally {
                loading.style.display = 'none';
                // Limpiar el campo de entrada despu√©s de una consulta exitosa
                document.getElementById('query').value = '';
            }
        }
        
        // Enfocar el campo de entrada al cargar la p√°gina
        window.onload = function() {
            document.getElementById('query').focus();
        }
    </script>
</body>
</html>
'''


# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

@app.route('/metrics')
def show_metrics():
    """Muestra m√©tricas en interfaz web"""
    html_table = metrics_logger.generate_comparative_table()
    return render_template_string(f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>M√©tricas - UFRO Assistant</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 10px; }}
            .back-button {{ background: #3498db; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üìä M√©tricas del Sistema</h1>
            <a href="/" class="back-button">‚Üê Volver al Chat</a>
        </div>
        {html_table}

  
    <div style="text-align: right; margin-bottom: 20px;">
        <a href="/metrics" style="background: #3498db; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-size: 14px;">
            üìä Ver M√©tricas
        </a>
    </div>
    </body>
    </html>
    """)

@app.route('/api/metrics/recent')
def api_recent_metrics():
    """API para obtener m√©tricas recientes"""
    recent = metrics_logger.get_recent_queries(10)
    return jsonify(recent)

@app.route('/api/metrics/stats')
def api_provider_stats():
    """API para obtener estad√≠sticas de proveedores"""
    stats = metrics_logger.get_provider_stats()
    return jsonify(stats)



class UFROAssistant:
    def __init__(self):
        self.providers = {}
        self.retriever = None
        self.generate_system_prompt = None
        
        try:
            # Importar dentro del try para manejar errores
            from providers.chatgpt import ChatGPTProvider
            from providers.deepseek import DeepSeekProvider
            from rag.retrieve import Retriever
            from rag.prompts import generate_system_prompt
            
            # Inicializar proveedores
            self.providers = {
                'chatgpt': ChatGPTProvider(),
                'deepseek': DeepSeekProvider()
            }
            
            # Inicializar retriever
            self.retriever = Retriever()
            
            # Guardar referencia a la funci√≥n
            self.generate_system_prompt = generate_system_prompt
            
            print("‚úÖ UFRO Assistant inicializado correctamente")
            print(f"‚úÖ Proveedores cargados: {list(self.providers.keys())}")
            
        except ImportError as e:
            print(f"‚ùå Error de importaci√≥n: {e}")
            print("‚ö†Ô∏è  Algunos componentes no est√°n disponibles")
        except Exception as e:
            print(f"‚ùå Error inicializando UFRO Assistant: {e}")
            # Mantener el sistema funcional con valores por defecto
            self.providers = {}
            self.retriever = None
    
    def is_initialized(self):
        """Verifica si el assistant est√° correctamente inicializado"""
        return (self.retriever is not None and 
                len(self.providers) > 0 and 
                self.generate_system_prompt is not None)
    
    def process_query(self, query: str, provider_name: str = 'chatgpt', k: int = 4) -> Dict:
        # ‚úÖ Usar import time global
        import time as time_module
        start_time = time_module.time()
        retrieval_start = time_module.time()
        
        # Verificar inicializaci√≥n
        if not self.is_initialized():
            return {
                'answer': '‚ö†Ô∏è El sistema no est√° completamente inicializado. Algunos componentes pueden no estar disponibles.',
                'citations': [],
                'latency': 0,
                'provider': 'none',
                'retrieval_latency': 0,
                'llm_latency': 0,
                'documents_retrieved': 0
            }
        
        # 1. Recuperar documentos relevantes
        retrieved_docs = self.retriever.search(query, k=k)
        retrieval_latency = time_module.time() - retrieval_start
        
        if not retrieved_docs:
            result = {
                'answer': 'No encontr√© informaci√≥n relevante en la normativa UFRO sobre este tema.',
                'citations': [],
                'latency': time_module.time() - start_time,
                'provider': provider_name,
                'retrieval_latency': round(retrieval_latency, 2),
                'llm_latency': 0,
                'documents_retrieved': 0
            }
            
            # Loggear m√©tricas
            self._log_metrics(query, result)
            return result
        
        # 2. Generar contexto
        context = "\n\n".join([
            f"[Documento: {doc.get('title', 'Unknown')}, P√°gina: {doc.get('page', 1)}]\n{doc.get('content', '')}"
            for doc in retrieved_docs
        ])
        
        # 3. Generar respuesta con el proveedor
        try:
            llm_start = time_module.time()
            
            provider = self.providers.get(provider_name)
            if not provider:
                # Fallback al primer proveedor disponible
                available_providers = list(self.providers.keys())
                if available_providers:
                    provider_name = available_providers[0]
                    provider = self.providers[provider_name]
                    print(f"‚ö†Ô∏è  Proveedor solicitado no disponible. Usando {provider_name} como fallback.")
                else:
                    result = {
                        'answer': '‚ùå No hay proveedores LLM disponibles en este momento.',
                        'citations': [],
                        'latency': time_module.time() - start_time,
                        'provider': 'none',
                        'retrieval_latency': round(retrieval_latency, 2),
                        'llm_latency': 0,
                        'documents_retrieved': len(retrieved_docs)
                    }
                    self._log_metrics(query, result)
                    return result
            
            # Usar la funci√≥n de prompt del sistema
            system_prompt = self.generate_system_prompt()
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Contexto:\n{context}\n\nPregunta: {query}"}
            ]
            
            response = provider.chat(messages)
            llm_latency = time_module.time() - llm_start
            
            # 4. Extraer citas
            citations = []
            for doc in retrieved_docs:
                citations.append(f"[{doc.get('title', 'Unknown')}, p.{doc.get('page', 1)}]")
            
            # 5. Crear resultado final
            total_latency = time_module.time() - start_time
            result = {
                'answer': response,
                'citations': citations[:3],
                'latency': round(total_latency, 2),
                'provider': provider_name,
                'retrieval_latency': round(retrieval_latency, 2),
                'llm_latency': round(llm_latency, 2),
                'documents_retrieved': len(retrieved_docs)
            }
            
            # 6. Loggear m√©tricas
            self._log_metrics(query, result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error procesando query: {e}")
            result = {
                'answer': f'‚ùå Error procesando la consulta: {str(e)}',
                'citations': [],
                'latency': round(time_module.time() - start_time, 2),
                'provider': provider_name,
                'retrieval_latency': round(retrieval_latency, 2),
                'llm_latency': 0,
                'documents_retrieved': len(retrieved_docs)
            }
            
            # Loggear incluso los errores
            self._log_metrics(query, result)
            
            return result
    
    def _log_metrics(self, query: str, result: Dict):
        """M√©todo helper para loggear m√©tricas"""
        try:
            from metrics_logger import metrics_logger
            metrics_data = {
                'query': query,
                'provider': result['provider'],
                'answer': result['answer'],
                'latency': result['latency'],
                'retrieval_latency': result.get('retrieval_latency', 0),
                'llm_latency': result.get('llm_latency', 0),
                'citations': result['citations'],
                'documents_retrieved': result.get('documents_retrieved', 0)
            }
            metrics_logger.log_query(metrics_data)
        except Exception as e:
            print(f"‚ö†Ô∏è Error loggeando m√©tricas: {e}")
    def _extract_citations(self, text: str) -> List[str]:
        """Extrae citas del formato [Documento, p√°gina X]"""
        import re
        citations = re.findall(r'\[([^,\]]+),\s*p√°gina?\s*(\d+)\]', text, re.IGNORECASE)
        return [f"{doc}, p.{page}" for doc, page in citations]

assistant = UFROAssistant()

@app.route('/')
def home():
    return render_template_string(HTML_TEMPLATE)

@app.route('/api/query', methods=['POST'])
def api_query():
    try:
        data = request.json
        query = data.get('query', '')
        provider = data.get('provider', 'chatgpt')
        
        if not query:
            return jsonify({'error': 'Query vac√≠a'}), 400
        
        result = assistant.process_query(query, provider)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Error procesando query: {e}")
        return jsonify({'error': str(e)}), 500

def cli_mode():
    """Modo CLI mejorado con UX profesional"""
    parser = argparse.ArgumentParser(description='ü§ñ UFRO Assistant - Sistema RAG de Normativa')
    parser.add_argument('query', nargs='?', help='Consulta sobre normativa UFRO')
    parser.add_argument('--provider', choices=['chatgpt', 'deepseek', 'auto'], default='deepseek', 
                       help='Proveedor LLM (auto=selecci√≥n autom√°tica)')
    parser.add_argument('--k', type=int, default=4, help='N√∫mero de documentos a recuperar (default: 4)')
    parser.add_argument('--batch', help='Archivo JSONL con preguntas para evaluaci√≥n batch')
    parser.add_argument('--web', action='store_true', help='Iniciar servidor web Flask')
    parser.add_argument('--stats', action='store_true', help='Mostrar estad√≠sticas del sistema')
    
    args = parser.parse_args()
    
    if args.stats:
        show_system_stats()
        return
    
    if args.web:
        start_web_server()
        return
    
    if args.batch:
        run_batch_evaluation(args)
        return
    
    if args.query:
        run_single_query(args)
    else:
        run_interactive_mode(args)


def show_system_stats():
    """Muestra estad√≠sticas del sistema"""
    print("üìä ESTAD√çSTICAS DEL SISTEMA UFRO ASSISTANT")
    print("=" * 50)
    
    try:
        from rag.retrieve import Retriever
        retriever = Retriever()
        stats = retriever.get_retrieval_stats()
        print("√çndice FAISS:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    except Exception as e:
        print(f"‚ùå Error obteniendo estad√≠sticas: {e}")

def start_web_server():
    """Inicia el servidor web Flask"""
    print("üöÄ Iniciando servidor Flask en http://0.0.0.0:5000")
    print("üì± Accede a la interfaz web: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=False)
def run_interactive_mode(args):
    """Modo interactivo"""
    print("ü§ñ UFRO Assistant - Modo interactivo")
    print("Escriba 'salir' para terminar\n")
    
    while True:
        try:
            query = input("Consulta: ").strip()
            if query.lower() in ['salir', 'exit', 'quit']:
                break
            
            if query:
                result = assistant.process_query(query, args.provider, args.k)
                print(f"\nRespuesta ({result['provider']}):")
                print(result['answer'])
                if result['citations']:
                    print(f"\nReferencias:")
                    for cite in result['citations']:
                        print(f"  ‚Ä¢ {cite}")
                print(f"\n[Tiempo: {result['latency']}s]")
                print("-" * 60)
                
        except KeyboardInterrupt:
            print("\nüëã ¬°Hasta luego!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")

def run_single_query(args):
    """Ejecuta una consulta √∫nica con formato profesional"""
    from rag.retrieve import Retriever
    from providers.router import ProviderRouter
    
    print("üîç UFRO Assistant - Procesando consulta...")
    print(f"üìù Consulta: {args.query}")
    print(f"ü§ñ Proveedor: {args.provider}")
    print(f"üìö Documentos a recuperar: {args.k}")
    print("-" * 60)
    
    start_time = time.time()
    
    try:
        retriever = Retriever()
        router = ProviderRouter()
        
        # Recuperar documentos
        docs = retriever.search(args.query, args.k)
        retrieval_time = time.time() - start_time
        
        print(f"‚úÖ Recuperados {len(docs)} documentos ({retrieval_time:.2f}s)")
        
        # Generar respuesta
        provider_to_use = None if args.provider == 'auto' else args.provider
        context = "\n".join([f"- {doc['title']} (p.{doc['page']}): {doc['content'][:200]}..." for doc in docs])
        
        messages = [
            {"role": "system", "content": generate_system_prompt()},
            {"role": "user", "content": f"Contexto:\n{context}\n\nPregunta: {args.query}"}
        ]
        
        response, provider, llm_time = router.chat(messages, provider_to_use)
        total_time = time.time() - start_time
        
        # Mostrar resultados
        print(f"\nü§ñ RESPUESTA ({provider.upper()}):")
        print("‚îÄ" * 60)
        print(response)
        print("‚îÄ" * 60)
        
        print(f"\nüìö REFERENCIAS:")
        for doc in docs[:3]:  # M√°ximo 3 referencias
            print(f"  ‚Ä¢ {doc['title']} - P√°gina {doc['page']} (score: {doc['score']:.3f})")
        
        print(f"\n‚è±Ô∏è  M√âTRICAS:")
        print(f"  ‚Ä¢ Tiempo total: {total_time:.2f}s")
        print(f"  ‚Ä¢ Recuperaci√≥n: {retrieval_time:.2f}s") 
        print(f"  ‚Ä¢ Generaci√≥n: {llm_time:.2f}s")
        print(f"  ‚Ä¢ Documentos recuperados: {len(docs)}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

def run_batch_evaluation(args):
    """Ejecuta evaluaci√≥n batch con manejo de errores mejorado"""
    print("üìä EJECUTANDO EVALUACI√ìN BATCH...")
    
    try:
        # Verificar que el archivo existe
        if not os.path.exists(args.batch):
            print(f"‚ùå Archivo no encontrado: {args.batch}")
            return
        
        with open(args.batch, 'r', encoding='utf-8') as f:
            questions = [json.loads(line) for line in f if line.strip()]
        
        if not questions:
            print("‚ùå No hay preguntas en el archivo batch")
            return
        
        print(f"üîç Procesando {len(questions)} preguntas...")
        
        results = []
        for i, q in enumerate(questions, 1):
            print(f"  {i}/{len(questions)}: {q.get('question', '')[:50]}...")
            
            try:
                result = assistant.process_query(q.get('question', ''), args.provider, args.k)
                
                # Evaluar calidad con manejo de errores
                try:
                    from eval.evaluate import Evaluator
                    evaluator = Evaluator()
                    metrics = evaluator.evaluate_response(result, q)
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  Error en evaluaci√≥n: {e}")
                    # M√©tricas por defecto si falla la evaluaci√≥n
                    metrics = {
                        'exact_match': 0.0,
                        'semantic_similarity': 0.0,
                        'citation_coverage': 1.0 if result['citations'] else 0.0,
                        'has_answer': len(result['answer'].strip()) > 0,
                        'answer_length': len(result['answer'])
                    }
                
                results.append({
                    'question': q.get('question', ''),
                    'expected_answer': q.get('expected_answer', ''),
                    'actual_answer': result['answer'],
                    'provider': result['provider'],
                    'citations': ', '.join(result['citations']),
                    'latency': result['latency'],
                    'retrieval_latency': result.get('retrieval_latency', 0),
                    'llm_latency': result.get('llm_latency', 0),
                    'exact_match': metrics.get('exact_match', 0),
                    'semantic_similarity': metrics.get('semantic_similarity', 0),
                    'citation_coverage': metrics.get('citation_coverage', 0),
                    'has_answer': metrics.get('has_answer', False),
                    'answer_length': metrics.get('answer_length', 0)
                })
                
            except Exception as e:
                print(f"    ‚ùå Error procesando pregunta {i}: {e}")
                # Registrar error pero continuar
                results.append({
                    'question': q.get('question', ''),
                    'expected_answer': q.get('expected_answer', ''),
                    'actual_answer': f'ERROR: {str(e)}',
                    'provider': 'error',
                    'citations': '',
                    'latency': 0,
                    'retrieval_latency': 0,
                    'llm_latency': 0,
                    'exact_match': 0,
                    'semantic_similarity': 0,
                    'citation_coverage': 0,
                    'has_answer': False,
                    'answer_length': 0
                })
        
        # Guardar CSV
        df = pd.DataFrame(results)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_results_{timestamp}.csv"
        df.to_csv(filename, index=False, encoding='utf-8')
        
        # Generar reporte
        generate_evaluation_report(df, filename)
        
        print(f"‚úÖ Evaluaci√≥n completada: {filename}")
        
    except Exception as e:
        print(f"‚ùå Error en evaluaci√≥n batch: {e}")
    
    print(f"‚úÖ Evaluaci√≥n completada: {filename}")
def show_metrics_cli():
        """Muestra m√©tricas en CLI"""
        print("üìä M√âTRICAS DEL SISTEMA UFRO ASSISTANT")
        print("=" * 60)
        
        # Generar y mostrar tabla comparativa
        html_table = metrics_logger.generate_comparative_table()
        
        # Extraer solo el contenido de la tabla para CLI
        try:
            import re
            # Extraer datos de la tabla HTML
            table_data = re.findall(r'<td>(.*?)</td>', html_table)
            
            if table_data:
                providers = set()
                current_provider = None
                print("\n" + "‚îÄ" * 100)
                print(f"{'Proveedor':<12} {'Consultas':<10} {'Latencia (s)':<12} {'Costo Avg':<12} {'Costo Total':<12} {'Tokens':<8} {'Citas':<8}")
                print("‚îÄ" * 100)
                
                # Leer datos del archivo CSV directamente
                stats_file = Path("logs/reportes/provider_stats.csv")
                if stats_file.exists():
                    df = pd.read_csv(stats_file)
                    for _, row in df.iterrows():
                        print(f"{row['provider']:<12} {row['total_queries']:<10} {row['avg_latency']:<12.2f} "
                            f"${row['avg_cost']:<11.6f} ${row['total_cost']:<11.6f} {row['avg_tokens']:<8.0f} {row.get('success_rate', 0):<8.1f}")
                
                print("‚îÄ" * 100)
                
            # Mostrar consultas recientes
            recent = metrics_logger.get_recent_queries(5)
            if recent:
                print(f"\nüïí √öLTIMAS 5 CONSULTAS:")
                for i, query in enumerate(recent, 1):
                    print(f"  {i}. [{query['provider']}] {query['query'][:50]}... "
                        f"(Latencia: {query['latency_total']:.2f}s, Costo: ${query['estimated_cost_usd']:.6f})")
                    
        except Exception as e:
            print(f"‚ùå Error mostrando m√©tricas: {e}")

        parser.add_argument('--metrics', action='store_true', help='Mostrar m√©tricas del sistema')

def generate_evaluation_report(df, filename):
    """Genera reporte de evaluaci√≥n ejecutivo"""
    report = f"""
üìä INFORME DE EVALUACI√ìN - UFRO ASSISTANT
==========================================
Fecha: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Muestras: {len(df)}
Archivo: {filename}

M√âTRICAS POR PROVEEDOR:
"""
    
    for provider in df['provider'].unique():
        provider_data = df[df['provider'] == provider]
        report += f"""
ü§ñ {provider.upper()}:
  ‚Ä¢ Exact Match: {provider_data['exact_match'].mean():.3f}
  ‚Ä¢ Similitud Sem√°ntica: {provider_data['semantic_similarity'].mean():.3f}
  ‚Ä¢ Cobertura de Citas: {provider_data['citation_coverage'].mean():.3f}
  ‚Ä¢ Latencia Promedio: {provider_data['latency'].mean():.2f}s
  ‚Ä¢ Muestras: {len(provider_data)}
"""
    
    # Hallazgos clave
    report += """
üîç HALLAZGOS PRINCIPALES:
"""
    
    if len(df['provider'].unique()) > 1:
        best_provider = df.groupby('provider')['semantic_similarity'].mean().idxmax()
        fastest_provider = df.groupby('provider')['latency'].mean().idxmin()
        
        report += f"  1. Mejor calidad: {best_provider.upper()}\n"
        report += f"  2. M√°s r√°pido: {fastest_provider.upper()}\n"
    
    report += f"  3. Cobertura de citas: {df['citation_coverage'].mean()*100:.1f}%\n"
    report += f"  4. Tiempo promedio: {df['latency'].mean():.2f}s\n"
    
    # Guardar reporte
    report_filename = filename.replace('.csv', '_report.txt')
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print(f"üìÑ Reporte guardado: {report_filename}")

if __name__ == '__main__':
    cli_mode()
